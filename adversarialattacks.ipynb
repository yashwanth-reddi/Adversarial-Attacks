{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZxTbAs5coOAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa7267b-07d0-40d2-b576-cb54253320c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-30 19:21:15--  https://uofi.box.com/shared/static/q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip\n",
            "Resolving uofi.box.com (uofi.box.com)... 74.112.186.144\n",
            "Connecting to uofi.box.com (uofi.box.com)|74.112.186.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip [following]\n",
            "--2023-05-30 19:21:15--  https://uofi.box.com/public/static/q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip\n",
            "Reusing existing connection to uofi.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://uofi.app.box.com/public/static/q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip [following]\n",
            "--2023-05-30 19:21:16--  https://uofi.app.box.com/public/static/q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip\n",
            "Resolving uofi.app.box.com (uofi.app.box.com)... 74.112.186.144\n",
            "Connecting to uofi.app.box.com (uofi.app.box.com)|74.112.186.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!6skidG7iuBUWiPEKwer1tKY46FnJR45FTPkQTiHdR7zbJqIri-Fu1Ov_w7h6kUaTVfcn907Dvn8QZtxDG9JLSj26_KMyldm8hfcq_ZZG7lVDDm0OQNXD9c4EGZoDPt4ugGTKhcklkC42QZux-5DLCb-LLBLcyFetbt-uc-wVe8hnFhLYrcFlpVO4OGqBjA9lnBeGkxMGSppvg4cKWRSS6a_P8RPGWT5fUD97Imn66u05TsMOxM6y4hgc7BIs03c9PwdTJ_CJU9e1x-Wu6SGnKWfypefDF6JwO7CtcDGRLICzkpmJEITx7PCEjeLHDSHrmGpvUFWPg8w8NdlClfLjAGz4X0Glraw7eSow0JTXhtLDOYKYc8L9QFahlUM9rvmUqmMJ4u_3GhJ1KTrJ_HzfmU0RNUSg79_wZx59mVkRowZsQGzfE7U-Z4uxOOZw61RZ6QejVPS1gmHU1x3v-eRf85lhSgxU0T_Eo0Xcm5yBUnK3V3zhnmR3Pufj1z78L-DKKJdBq_c5UEhsP0NEM9fGx7w9j4yneIw0CZTbCz60jzXYkK_q-0xQlfiei1VhEA8D9mI5KuiYr88wEkEfxvN9IPYk-l-s3btZQQ-EBDP4Hnz0YAeBunbJwDeU3ioYJ_ssa48IYZvML3uX61X_q-0tarkEQVzv3AsNQLivo6YS9aHExL0j4lYoPcCR0-KbwFyVQmbVzNauoV2S009wv4Ct7xZmkY3HwA5mqBGRL4vNYtH-T_Bra4NtGj0Qgqc-a8sGlPC_b1Ew1eGBKScSvCoLZd1anGZSj48zNd4AQpWcMurwh_kZHI40BqH9KAyKC6-iltw24sTq5jUSOog2asLYTqV0Eyq_HtXAmlU0pyBZOP-U621vgnmOaujEEqnSO40Ti50WR6htoOZROPb4Zky6Rh-XJ7bDrNZjkxMxt0cv-N-NWFFe_N8DKP2Rb86zZmweV_hKSVgxs5l9JYZqyORwljTIBsxlcIfRoWlB_rLW2ckZFf2NXDH3ki7_HLBanTiRe39eke4A_2-E1vDgoWQQ2J-feX5qQHiFcFdI7_zXegf9_pbz3zDi6QgTMtWHr6g5kTVxjYABoxFl3nuMv2zINXPoxgsJuFE-uefsc1FpxPlXYIp65g0g9KYSDMm6Xys6LqJPXB0Y3OFc2rgc7LcUGnfP0v1APOOxRl3it2lyqpQdrwEkXy16idLqlUOnJVxLclcy2mZ_kbJ1dGUcq9N8gfUybkYOzHYeWeco7PViKnQlYUgW5_EBNMT0kjzJIPktJTR0HvJt8bAJmcR_WoHLcwi-tUKYFOs-B8lNZvt06Etj9pvIKVv0OtdTJ35BL8DoKdRbW_93_GweUl1kpVB_PvQHVMbaiUoBqgsZm7RkD7wUGTIPOr0u5MIAYe74mnwVMQA./download [following]\n",
            "--2023-05-30 19:21:17--  https://public.boxcloud.com/d/1/b1!6skidG7iuBUWiPEKwer1tKY46FnJR45FTPkQTiHdR7zbJqIri-Fu1Ov_w7h6kUaTVfcn907Dvn8QZtxDG9JLSj26_KMyldm8hfcq_ZZG7lVDDm0OQNXD9c4EGZoDPt4ugGTKhcklkC42QZux-5DLCb-LLBLcyFetbt-uc-wVe8hnFhLYrcFlpVO4OGqBjA9lnBeGkxMGSppvg4cKWRSS6a_P8RPGWT5fUD97Imn66u05TsMOxM6y4hgc7BIs03c9PwdTJ_CJU9e1x-Wu6SGnKWfypefDF6JwO7CtcDGRLICzkpmJEITx7PCEjeLHDSHrmGpvUFWPg8w8NdlClfLjAGz4X0Glraw7eSow0JTXhtLDOYKYc8L9QFahlUM9rvmUqmMJ4u_3GhJ1KTrJ_HzfmU0RNUSg79_wZx59mVkRowZsQGzfE7U-Z4uxOOZw61RZ6QejVPS1gmHU1x3v-eRf85lhSgxU0T_Eo0Xcm5yBUnK3V3zhnmR3Pufj1z78L-DKKJdBq_c5UEhsP0NEM9fGx7w9j4yneIw0CZTbCz60jzXYkK_q-0xQlfiei1VhEA8D9mI5KuiYr88wEkEfxvN9IPYk-l-s3btZQQ-EBDP4Hnz0YAeBunbJwDeU3ioYJ_ssa48IYZvML3uX61X_q-0tarkEQVzv3AsNQLivo6YS9aHExL0j4lYoPcCR0-KbwFyVQmbVzNauoV2S009wv4Ct7xZmkY3HwA5mqBGRL4vNYtH-T_Bra4NtGj0Qgqc-a8sGlPC_b1Ew1eGBKScSvCoLZd1anGZSj48zNd4AQpWcMurwh_kZHI40BqH9KAyKC6-iltw24sTq5jUSOog2asLYTqV0Eyq_HtXAmlU0pyBZOP-U621vgnmOaujEEqnSO40Ti50WR6htoOZROPb4Zky6Rh-XJ7bDrNZjkxMxt0cv-N-NWFFe_N8DKP2Rb86zZmweV_hKSVgxs5l9JYZqyORwljTIBsxlcIfRoWlB_rLW2ckZFf2NXDH3ki7_HLBanTiRe39eke4A_2-E1vDgoWQQ2J-feX5qQHiFcFdI7_zXegf9_pbz3zDi6QgTMtWHr6g5kTVxjYABoxFl3nuMv2zINXPoxgsJuFE-uefsc1FpxPlXYIp65g0g9KYSDMm6Xys6LqJPXB0Y3OFc2rgc7LcUGnfP0v1APOOxRl3it2lyqpQdrwEkXy16idLqlUOnJVxLclcy2mZ_kbJ1dGUcq9N8gfUybkYOzHYeWeco7PViKnQlYUgW5_EBNMT0kjzJIPktJTR0HvJt8bAJmcR_WoHLcwi-tUKYFOs-B8lNZvt06Etj9pvIKVv0OtdTJ35BL8DoKdRbW_93_GweUl1kpVB_PvQHVMbaiUoBqgsZm7RkD7wUGTIPOr0u5MIAYe74mnwVMQA./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 881817696 (841M) [application/zip]\n",
            "Saving to: ‘q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip’\n",
            "\n",
            "wofuulhhphjj.zip     36%[======>             ] 304.57M  9.31MB/s    eta 60s    ^C\n",
            "Archive:  q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip or\n",
            "        q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip.zip, and cannot find q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!wget https://uofi.box.com/shared/static/q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip\n",
        "!mkdir celeba_data\n",
        "!unzip q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip -d celeba_data\n",
        "!rm q4pf89jtkvjndi4f8ip7wofuulhhphjj.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1MRdURuyIbg5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "def show_images(images, color=False):\n",
        "    if color:\n",
        "        sqrtimg = int(np.ceil(np.sqrt(images.shape[2]*images.shape[3])))\n",
        "    else:\n",
        "        images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
        "        sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
        "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
        "\n",
        "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
        "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        if color:\n",
        "            plt.imshow(np.swapaxes(np.swapaxes(img, 0, 1), 1, 2))\n",
        "        else:\n",
        "            plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
        "    return \n",
        "\n",
        "def preprocess_img(x):\n",
        "    return 2 * x - 1.0\n",
        "\n",
        "def deprocess_img(x):\n",
        "    return (x + 1.0) / 2.0\n",
        "\n",
        "def rel_error(x,y):\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "def sample_noise(batch_size, dim):\n",
        "    \"\"\"\n",
        "    Generate a PyTorch Tensor of uniform random noise.\n",
        "\n",
        "    Input:\n",
        "    - batch_size: Integer giving the batch size of noise to generate.\n",
        "    - dim: Integer giving the dimension of noise to generate.\n",
        "    \n",
        "    Output:\n",
        "    - A PyTorch Tensor of shape (batch_size, dim) containing uniform\n",
        "      random noise in the range (-1, 1).\n",
        "    \"\"\"\n",
        "    to_return = torch.randn((batch_size, dim))\n",
        "    return to_return/torch.max(to_return)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fl1gMsgVjHhB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=2):\n",
        "        \"\"\"\n",
        "        Reference:\n",
        "        SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS: https://arxiv.org/pdf/1802.05957.pdf\n",
        "        \"\"\"\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        \"\"\"\n",
        "        Implement Spectral Normalization\n",
        "        Hint: 1: Use getattr to first extract u, v, w.\n",
        "              2: Apply power iteration.\n",
        "              3: Calculate w with the spectral norm.\n",
        "              4: Use setattr to update w in the module.\n",
        "        \"\"\"\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "\n",
        "        reshapedw = w.view(w.size(0), -1)\n",
        "\n",
        "        for i in range(self.power_iterations):\n",
        "          mulwu = torch.mv(torch.t(reshapedw), u)\n",
        "          mulwv = torch.mv(reshapedw, v)\n",
        "          v = l2normalize(mulwu)\n",
        "          u = l2normalize(mulwv)\n",
        "\n",
        "        sigma = u.dot(reshapedw.mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "        \n",
        "\n",
        "    def _make_params(self):\n",
        "        \"\"\"\n",
        "        No need to change. Initialize parameters.\n",
        "        v: Initialize v with a random vector (sampled from isotropic distrition).\n",
        "        u: Initialize u with a random vector (sampled from isotropic distrition).\n",
        "        w: Weight of the current layer.\n",
        "        \"\"\"\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        \"\"\"\n",
        "        No need to change. Update weights using spectral normalization.\n",
        "        \"\"\"\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nA2rg_tljL9c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def discriminator_loss(logits_real, logits_fake):\n",
        "    \"\"\"\n",
        "    Computes the discriminator loss.\n",
        "    \n",
        "    You should use the stable torch.nn.functional.binary_cross_entropy_with_logits \n",
        "    loss rather than using a separate softmax function followed by the binary cross\n",
        "    entropy loss.\n",
        "    \n",
        "    Inputs:\n",
        "    - logits_real: PyTorch Tensor of shape (N,) giving scores for the real data.\n",
        "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
        "    \n",
        "    Returns:\n",
        "    - loss: PyTorch Tensor containing (scalar) the loss for the discriminator.\n",
        "    \"\"\"\n",
        "    \n",
        "    onse =  torch.ones_like(logits_real)\n",
        "    zeros = torch.zeros_like(logits_fake)\n",
        "\n",
        "    loss_real = torch.nn.functional.binary_cross_entropy_with_logits (logits_real,onse)\n",
        "    loss_fake = torch.nn.functional.binary_cross_entropy_with_logits (logits_fake,zeros )\n",
        "    \n",
        "    loss = loss_real + loss_fake\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def generator_loss(logits_fake):\n",
        "    \"\"\"\n",
        "    Computes the generator loss.\n",
        "    \n",
        "    You should use the stable torch.nn.functional.binary_cross_entropy_with_logits \n",
        "    loss rather than using a separate softmax function followed by the binary cross\n",
        "    entropy loss.\n",
        "\n",
        "    Inputs:\n",
        "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
        "    \n",
        "    Returns:\n",
        "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
        "    \"\"\"\n",
        "    \n",
        "    onse = torch.ones_like(logits_fake)\n",
        "    loss = bce_loss(logits_fake, onse)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def ls_discriminator_loss(scores_real, scores_fake):\n",
        "    \"\"\"\n",
        "    Compute the Least-Squares GAN loss for the discriminator.\n",
        "    \n",
        "    Inputs:\n",
        "    - scores_real: PyTorch Tensor of shape (N,) giving scores for the real data.\n",
        "    - scores_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
        "    \n",
        "    Outputs:\n",
        "    - loss: A PyTorch Tensor containing the loss.\n",
        "    \"\"\"\n",
        "    onse = torch.ones_like(scores_real)\n",
        "    zeros = torch.zeros_like(scores_fake)\n",
        "\n",
        "    loss = ((torch.nn.functional.mse_loss(scores_real, onse))/2 )+ ((torch.nn.functional.mse_loss(scores_fake, zeros))/2)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def ls_generator_loss(scores_fake):\n",
        "    \"\"\"\n",
        "    Computes the Least-Squares GAN loss for the generator.\n",
        "    \n",
        "    Inputs:\n",
        "    - scores_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
        "    \n",
        "    Outputs:\n",
        "    - loss: A PyTorch Tensor containing the loss.\n",
        "    \"\"\"\n",
        "    onse = torch.ones_like(scores_fake)\n",
        "\n",
        "    loss = (torch.nn.functional.mse_loss(scores_fake, onse))/2\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hEBJ2E9mjMXW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#from spectral_normalization import SpectralNorm\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "  def __init__(self, input_channels=3):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.conv1 = SpectralNorm(nn.Conv2d(input_channels, 128, 4, 2, 1))\n",
        "    self.conv2 = SpectralNorm(nn.Conv2d(128, 256, 4, 2, 1))\n",
        "    self.conv3 = SpectralNorm(nn.Conv2d(256, 512, 4, 2, 1))\n",
        "    self.conv4 = SpectralNorm(nn.Conv2d(512, 1024, 4, 2, 1))\n",
        "    self.conv5 = SpectralNorm(nn.Conv2d(1024, 1, 4, 1, 0))\n",
        "    self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.leaky_relu(self.conv1(x))\n",
        "    x = self.leaky_relu(self.conv2(x))\n",
        "    x = self.leaky_relu(self.conv3(x))\n",
        "    x = self.leaky_relu(self.conv4(x))\n",
        "    x = self.conv5(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "  def __init__(self, noise_dim, output_channels=3):\n",
        "    super(Generator, self).__init__()    \n",
        "    self.noise_dim = noise_dim\n",
        "    self.linear = nn.Linear(self.noise_dim, 4*4*1024)\n",
        "    self.conv1 = nn.ConvTranspose2d(1024, 512, 4, 2, 1)\n",
        "    self.conv2 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
        "    self.conv3 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
        "    self.conv4 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
        "    self.conv5 = nn.ConvTranspose2d(64, output_channels, 4, 2, 1)\n",
        "    self.batch_norm0 = nn.BatchNorm2d(512)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(256)\n",
        "    self.batch_norm2 = nn.BatchNorm2d(128)\n",
        "    self.batch_norm3 = nn.BatchNorm2d(64)\n",
        "    # self.batch_norm4 = nn.BatchNorm2d(32)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.tanh = nn.Tanh()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = x.view(-1, 1024, 4, 4)\n",
        "    x = self.relu(self.batch_norm0(self.conv1(x)))\n",
        "    x = self.relu(self.batch_norm1(self.conv2(x)))\n",
        "    x = self.relu(self.batch_norm2(self.conv3(x)))\n",
        "    x = self.relu(self.batch_norm3(self.conv4(x)))\n",
        "    x = self.tanh(self.conv5(x))\n",
        "    \n",
        "    return x\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "256Q8Gw1rwfC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# from gan.utils import sample_noise, show_images, deprocess_img, preprocess_img\n",
        "\n",
        "def train(D, G, D_solver, G_solver, discriminator_loss, generator_loss, show_every=250, \n",
        "              batch_size=128, noise_size=100, num_epochs=10, train_loader=None, device=None):\n",
        "    \"\"\"\n",
        "    Train loop for GAN.\n",
        "    \n",
        "    The loop will consist of two steps: a discriminator step and a generator step.\n",
        "    \n",
        "    (1) In the discriminator step, you should zero gradients in the discriminator \n",
        "    and sample noise to generate a fake data batch using the generator. Calculate \n",
        "    the discriminator output for real and fake data, and use the output to compute\n",
        "    discriminator loss. Call backward() on the loss output and take an optimizer\n",
        "    step for the discriminator.\n",
        "    \n",
        "    (2) For the generator step, you should once again zero gradients in the generator\n",
        "    and sample noise to generate a fake data batch. Get the discriminator output\n",
        "    for the fake data batch and use this to compute the generator loss. Once again\n",
        "    call backward() on the loss and take an optimizer step.\n",
        "    \n",
        "    You will need to reshape the fake image tensor outputted by the generator to \n",
        "    be dimensions (batch_size x input_channels x img_size x img_size).\n",
        "    \n",
        "    Use the sample_noise function to sample random noise, and the discriminator_loss\n",
        "    and generator_loss functions for their respective loss computations.\n",
        "    \n",
        "    \n",
        "    Inputs:\n",
        "    - D, G: PyTorch models for the discriminator and generator\n",
        "    - D_solver, G_solver: torch.optim Optimizers to use for training the\n",
        "      discriminator and generator.\n",
        "    - discriminator_loss, generator_loss: Functions to use for computing the generator and\n",
        "      discriminator loss, respectively.\n",
        "    - show_every: Show samples after every show_every iterations.\n",
        "    - batch_size: Batch size to use for training.\n",
        "    - noise_size: Dimension of the noise to use as input to the generator.\n",
        "    - num_epochs: Number of epochs over the training dataset to use for training.\n",
        "    - train_loader: image dataloader\n",
        "    - device: PyTorch device\n",
        "    \"\"\"\n",
        "    dgraph = []\n",
        "    ggraph = []\n",
        "\n",
        "    iter_count = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('EPOCH: ', (epoch+1))\n",
        "        for x, _ in train_loader:\n",
        "            _, input_channels, img_size, _ = x.shape\n",
        "            \n",
        "            real_images = preprocess_img(x).to(device)  # normalize\n",
        "            \n",
        "            # Store discriminator loss output, generator loss output, and fake image output\n",
        "            # in these variables for logging and visualization below\n",
        "            d_error = None\n",
        "            g_error = None\n",
        "            fake_images = None\n",
        "\n",
        "            D_solver.zero_grad()\n",
        "            noise = sample_noise(batch_size, noise_size).to(device)\n",
        "            d_error = discriminator_loss(D(real_images), D(G(noise)))\n",
        "            dgraph.append(d_error.item())\n",
        "            d_error.backward()\n",
        "            D_solver.step()\n",
        "\n",
        "            G_solver.zero_grad()\n",
        "            noise = sample_noise(batch_size, noise_size).to(device)\n",
        "            fake_images = G(noise)\n",
        "            g_error = generator_loss(D(fake_images))\n",
        "            ggraph.append(g_error.item())\n",
        "            g_error.backward()\n",
        "            G_solver.step()\n",
        "\n",
        "            if (iter_count % show_every == 0):\n",
        "                print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count,d_error.item(),g_error.item()))\n",
        "                disp_fake_images = deprocess_img(fake_images.data)  # denormalize\n",
        "                imgs_numpy = (disp_fake_images).cpu().numpy()\n",
        "                show_images(imgs_numpy[0:16], color=input_channels!=1)\n",
        "                plt.show()\n",
        "                print()\n",
        "            iter_count += 1\n",
        "\n",
        "    return dgraph,ggraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8N8_ge9cDg6"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "For this part of the assignment you implement two different types of generative adversarial networks. We will train the networks on the Celeb A dataset which is a large set of celebrity face images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QRNx4wtGvPYR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bh5rbR7DcDhH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpsWMANcDhJ"
      },
      "source": [
        "# GAN loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WhpLQvPcDhK"
      },
      "source": [
        "In this assignment you will implement two different types of GAN cost functions. You will first implement the loss from the [original GAN paper](https://arxiv.org/pdf/1406.2661.pdf). You will also implement the loss from [LS-GAN](https://arxiv.org/abs/1611.04076). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOVWxdbK-nuI"
      },
      "source": [
        "### GAN loss\n",
        "\n",
        "**TODO:** Implement the `discriminator_loss` and `generator_loss` functions in `gan/losses.py`.\n",
        "\n",
        "The generator loss is given by:\n",
        "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
        "and the discriminator loss is:\n",
        "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
        "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses.\n",
        "\n",
        "**HINTS**: You should use the `torch.nn.functional.binary_cross_entropy_with_logits` function to compute the binary cross entropy loss since it is more numerically stable than using a softmax followed by BCE loss. The BCE loss is needed to compute the log probability of the true label given the logits output from the discriminator. Given a score $s\\in\\mathbb{R}$ and a label $y\\in\\{0, 1\\}$, the binary cross entropy loss is\n",
        "\n",
        "$$ bce(s, y) = -y * \\log(s) - (1 - y) * \\log(1 - s) $$\n",
        "\n",
        "\n",
        "Instead of computing the expectation of $\\log D(G(z))$, $\\log D(x)$ and $\\log \\left(1-D(G(z))\\right)$, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KQkURZmncDhL"
      },
      "outputs": [],
      "source": [
        "# from gan.losses import discriminator_loss, generator_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXfoe96P-nud"
      },
      "source": [
        "### Least Squares GAN loss\n",
        "\n",
        "**TODO:** Implement the `ls_discriminator_loss` and `ls_generator_loss` functions in `gan/losses.py`.\n",
        "\n",
        "We'll now look at [Least Squares GAN](https://arxiv.org/abs/1611.04076), a newer, more stable alernative to the original GAN loss function. For this part, all we have to do is change the loss function and retrain the model. We'll implement equation (9) in the paper, with the generator loss:\n",
        "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
        "and the discriminator loss:\n",
        "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$\n",
        "\n",
        "\n",
        "**HINTS**: Instead of computing the expectation, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing. When plugging in for $D(x)$ and $D(G(z))$ use the direct output from the discriminator (`scores_real` and `scores_fake`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gcbHKxg6cDhO"
      },
      "outputs": [],
      "source": [
        "# from gan.losses import ls_discriminator_loss, ls_generator_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D4NN3g1cDhR"
      },
      "source": [
        "# GAN model architecture\n",
        "\n",
        "**TODO:** Implement the `Discriminator` and `Generator` networks in `gan/models.py`.\n",
        "\n",
        "We recommend the following architectures which are inspired by [DCGAN](https://arxiv.org/pdf/1511.06434.pdf):\n",
        "\n",
        "**Discriminator:**\n",
        "\n",
        "- convolutional layer with in_channels=3, out_channels=128, kernel=4, stride=2\n",
        "- convolutional layer with in_channels=128, out_channels=256, kernel=4, stride=2\n",
        "- batch norm\n",
        "- convolutional layer with in_channels=256, out_channels=512, kernel=4, stride=2\n",
        "- batch norm\n",
        "- convolutional layer with in_channels=512, out_channels=1024, kernel=4, stride=2\n",
        "- batch norm\n",
        "- convolutional layer with in_channels=1024, out_channels=1, kernel=4, stride=1\n",
        "\n",
        "Use padding = 1 (not 0) for all the convolutional layers.\n",
        "\n",
        "Instead of Relu we LeakyReLu throughout the discriminator (we use a negative slope value of 0.2). You can use simply use relu as well.\n",
        "\n",
        "The output of your discriminator should be a single value score corresponding to each input sample. See `torch.nn.LeakyReLU`.\n",
        "\n",
        "\n",
        "**Generator:**\n",
        "\n",
        "**Note:** In the generator, you will need to use transposed convolution (sometimes known as fractionally-strided convolution or deconvolution). This function is implemented in pytorch as `torch.nn.ConvTranspose2d`.\n",
        "\n",
        "- transpose convolution with in_channels=NOISE_DIM, out_channels=1024, kernel=4, stride=1\n",
        "- batch norm\n",
        "- transpose convolution with in_channels=1024, out_channels=512, kernel=4, stride=2\n",
        "- batch norm\n",
        "- transpose convolution with in_channels=512, out_channels=256, kernel=4, stride=2\n",
        "- batch norm\n",
        "- transpose convolution with in_channels=256, out_channels=128, kernel=4, stride=2\n",
        "- batch norm\n",
        "- transpose convolution with in_channels=128, out_channels=3, kernel=4, stride=2\n",
        "\n",
        "The output of the final layer of the generator network should have a `tanh` nonlinearity to output values between -1 and 1. The output should be a 3x64x64 tensor for each sample (equal dimensions to the images from the dataset).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2GJ79-qNcDhR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#from spectral_normalization import SpectralNorm\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "  def __init__(self, input_channels=3):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.conv1 = SpectralNorm(nn.Conv2d(input_channels, 128, 4, 2, 1))\n",
        "    self.conv2 = SpectralNorm(nn.Conv2d(128, 256, 4, 2, 1))\n",
        "    self.conv3 = SpectralNorm(nn.Conv2d(256, 512, 4, 2, 1))\n",
        "    self.conv4 = SpectralNorm(nn.Conv2d(512, 1024, 4, 2, 1))\n",
        "    self.conv5 = SpectralNorm(nn.Conv2d(1024, 1, 4, 1, 0))\n",
        "    self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.leaky_relu(self.conv1(x))\n",
        "    x = self.leaky_relu(self.conv2(x))\n",
        "    x = self.leaky_relu(self.conv3(x))\n",
        "    x = self.leaky_relu(self.conv4(x))\n",
        "    x = self.conv5(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "  def __init__(self, noise_dim, output_channels=3):\n",
        "    super(Generator, self).__init__()    \n",
        "    self.noise_dim = noise_dim\n",
        "    self.linear = nn.Linear(self.noise_dim, 4*4*1024)\n",
        "    self.conv1 = nn.ConvTranspose2d(1024, 512, 4, 2, 1)\n",
        "    self.conv2 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
        "    self.conv3 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
        "    self.conv4 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
        "    self.conv5 = nn.ConvTranspose2d(64, output_channels, 4, 2, 1)\n",
        "    self.batch_norm0 = nn.BatchNorm2d(512)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(256)\n",
        "    self.batch_norm2 = nn.BatchNorm2d(128)\n",
        "    self.batch_norm3 = nn.BatchNorm2d(64)\n",
        "    # self.batch_norm4 = nn.BatchNorm2d(32)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.tanh = nn.Tanh()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = x.view(-1, 1024, 4, 4)\n",
        "    x = self.relu(self.batch_norm0(self.conv1(x)))\n",
        "    x = self.relu(self.batch_norm1(self.conv2(x)))\n",
        "    x = self.relu(self.batch_norm2(self.conv3(x)))\n",
        "    x = self.relu(self.batch_norm3(self.conv4(x)))\n",
        "    x = self.tanh(self.conv5(x))\n",
        "    \n",
        "    return x\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sgpDWlbM90t"
      },
      "source": [
        "# Data loading: Celeb A Dataset\n",
        "\n",
        "The CelebA images we provide have been filtered to obtain only images with clear faces and have been cropped and downsampled to 128x128 resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XRDbz7oxcDhU"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "scale_size = 128  # We resize the images to 64x64 for training\n",
        "\n",
        "celeba_root = 'celeba_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a6IzyDdZM9bp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "55867f6e-589d-414d-d95f-5e6bd51d151a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-277f60feeec7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m celeba_train = ImageFolder(root=celeba_root, transform=transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in celeba_data."
          ]
        }
      ],
      "source": [
        "celeba_train = ImageFolder(root=celeba_root, transform=transforms.Compose([\n",
        "  transforms.Resize(scale_size),\n",
        "  transforms.ToTensor(),\n",
        "]))\n",
        "\n",
        "celeba_loader_train = DataLoader(celeba_train, batch_size=batch_size, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZYAYVoLtBm6"
      },
      "outputs": [],
      "source": [
        "len(celeba_loader_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1JkDB-wcDhY"
      },
      "source": [
        "### Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1W4x7FiX5kL"
      },
      "outputs": [],
      "source": [
        "celeba_loader_train\n",
        "\n",
        "images, labels = next(iter(celeba_loader_train))\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# here shape is 3X128X128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiThPjUSwc3P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# from gan.utils import show_images\n",
        "\n",
        "imgs = next(iter(celeba_loader_train))[0].numpy()\n",
        "\n",
        "show_images(imgs, color=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM-RXjszcDhb"
      },
      "source": [
        "# Training \n",
        "\n",
        "\n",
        "**TODO:** Fill in the training loop in `gan/train.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV3Ic7e2cDhb"
      },
      "outputs": [],
      "source": [
        "NOISE_DIM = 256\n",
        "NUM_EPOCHS = 20\n",
        "learning_rate = 0.0002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYq7hQXicDhe"
      },
      "source": [
        "### Train GAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "celeba_loader_train\n",
        "\n",
        "images, labels = next(iter(celeba_loader_train))\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# here shape is 3X128X128\n",
        "# rgb with 128 x 128 pixels "
      ],
      "metadata": {
        "id": "y4JhxjR-aeBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "imgs_numpy = next(iter(celeba_loader_train))[0].numpy()\n",
        "\n",
        "k=127\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "temp = imgs_numpy[k]\n",
        "    \n",
        "image_data = np.transpose(temp, (1, 2, 0))\n",
        "ax.imshow(image_data)\n",
        "plt.show()\n",
        "\n",
        "print(\"the image shape is 3 x 128 x128\")"
      ],
      "metadata": {
        "id": "DkSyNGCdbaEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbcFiz0pI1yF",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "D = Discriminator().to(device)\n",
        "G = Generator(noise_dim=NOISE_DIM).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGXt8PTtcDhg"
      },
      "outputs": [],
      "source": [
        "D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate, betas = (0.5, 0.999))\n",
        "G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate, betas = (0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVD3zfFnG6e0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# original gan\n",
        "dgraph,ggraph =  train(D, G, D_optimizer, G_optimizer, discriminator_loss, generator_loss,batch_size=128,noise_size=256, num_epochs=NUM_EPOCHS, show_every=150,train_loader=celeba_loader_train, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EULELT-tTGf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "plt.plot(dgraph, color='green', linestyle='dashed', linewidth = 1,\n",
        "         marker='o', markerfacecolor='blue', markersize=1)\n",
        "\n",
        "plt.plot(ggraph, color='red', linestyle='dashed', linewidth = 1,\n",
        "         marker='o', markerfacecolor='blue', markersize=1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnlOaM3tcDhl"
      },
      "source": [
        "### Train LS-GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69X2hCfacDhm"
      },
      "outputs": [],
      "source": [
        "D = Discriminator().to(device)\n",
        "G = Generator(noise_dim=NOISE_DIM).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pmRgmMNcDho"
      },
      "outputs": [],
      "source": [
        "D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate, betas = (0.5, 0.999))\n",
        "G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate, betas = (0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK21R3OcDhq"
      },
      "outputs": [],
      "source": [
        "# ls-gan\n",
        "lsdgraph,lsggraph = train(D, G, D_optimizer, G_optimizer, ls_discriminator_loss, ls_generator_loss,batch_size=128, num_epochs=NUM_EPOCHS, show_every=200,noise_size=256,train_loader=celeba_loader_train, device=device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj3zhGgScDhs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "plt.plot(lsdgraph, color='green', linestyle='dashed', linewidth = 1,\n",
        "         marker='o', markerfacecolor='blue', markersize=1)\n",
        "\n",
        "plt.plot(lsggraph, color='red', linestyle='dashed', linewidth = 1,\n",
        "         marker='o', markerfacecolor='blue', markersize=1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbSdxWuWtjtL"
      },
      "outputs": [],
      "source": [
        "noise = sample_noise(batch_size, 256).to(device)\n",
        "fake_images = G(noise)\n",
        "disp_fake_images = deprocess_img(fake_images.data)  # denormalize\n",
        "imgs_numpy = (disp_fake_images).cpu().numpy()\n",
        "show_images(imgs_numpy[0:16], color=3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ZMKCjTaBsL"
      },
      "outputs": [],
      "source": [
        "print(imgs_numpy.shape)\n",
        "k=4\n",
        "%matplotlib inline\n",
        "show_images(imgs_numpy[k:k+1], color=3)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "temp = imgs_numpy[k]\n",
        "    \n",
        "image_data = np.transpose(temp, (1, 2, 0))\n",
        "ax.imshow(image_data)\n",
        "plt.show()\n",
        "\n",
        "print(\"the image shape is 3 x 128 x128\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1otXYjPxaBsL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miYFfqbGaBsL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}